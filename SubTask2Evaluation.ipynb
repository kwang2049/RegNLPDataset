{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1dCSh6IyOZxUoHwnlgnUUVDpNKfrvDqrA",
      "authorship_tag": "ABX9TyOhYZHAkmGO7DxflyWpenEA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tubagokhan/RegNLPDataset/blob/main/SubTask2Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4sWr8-rN_Vm",
        "outputId": "86f425f7-958d-4410-9ca0-24835e7060da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-2.5.1-py3-none-any.whl (156 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/156.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/156.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence_transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 sentence_transformers-2.5.1\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# Define the mapping from numerical predictions to labels\n",
        "label_mapping = ['contradiction', 'entailment', 'neutral']\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-deberta-v3-large')\n",
        "tokenizer = AutoTokenizer.from_pretrained('cross-encoder/nli-deberta-v3-large')\n",
        "\n",
        "# Load the JSON data\n",
        "with open('/content/drive/Othercomputers/MBZUAI/MBZUAI/ADGM-Project/SharedTask/COBS_VER15.150823_answer_entailed_questions_subtask2_sample.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "item_count = 0  # Initialize item counter\n",
        "\n",
        "# Iterate through each question-answer pair\n",
        "for item in data:\n",
        "    premise = item['Question']\n",
        "    hypothesis = item['Answer']\n",
        "\n",
        "    # Tokenize the premise and hypothesis\n",
        "    features = tokenizer(premise, hypothesis, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Perform inference\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**features)\n",
        "    print(outputs)\n",
        "    # Get the predicted class and map it to the corresponding label\n",
        "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
        "    prediction_label = label_mapping[prediction]\n",
        "    print(prediction_label)\n",
        "    # Add the prediction label to the item\n",
        "    item['Prediction'] = prediction_label\n",
        "\n",
        "    item_count += 1  # Increment the counter\n",
        "\n",
        "    print(f\"Total items processed: {item_count}\")\n",
        "\n",
        "# Save the updated data back to a new JSON file\n",
        "with open('/content/drive/Othercomputers/MBZUAI/MBZUAI/ADGM-Project/SharedTask/COBS_VER15.150823_answer_entailed_questions_subtask2_sample_with_labels.json', 'w') as outfile:\n",
        "    json.dump(data, outfile, indent=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDf_a4hWOHVK",
        "outputId": "fa47e859-1220-4974-ba88-e3466853fd20"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SequenceClassifierOutput(loss=None, logits=tensor([[-4.7668, -0.5329,  4.1622]]), hidden_states=None, attentions=None)\n",
            "neutral\n",
            "Total items processed: 1\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[-0.4749, -1.9692,  1.6881]]), hidden_states=None, attentions=None)\n",
            "neutral\n",
            "Total items processed: 2\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[-3.5845, -3.0653,  4.9062]]), hidden_states=None, attentions=None)\n",
            "neutral\n",
            "Total items processed: 3\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[-4.7853,  1.0738,  2.8910]]), hidden_states=None, attentions=None)\n",
            "neutral\n",
            "Total items processed: 4\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[-3.4108, -2.3640,  4.3611]]), hidden_states=None, attentions=None)\n",
            "neutral\n",
            "Total items processed: 5\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[-3.9342,  2.9345,  0.9419]]), hidden_states=None, attentions=None)\n",
            "entailment\n",
            "Total items processed: 6\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[-5.3630,  2.3814,  2.4729]]), hidden_states=None, attentions=None)\n",
            "neutral\n",
            "Total items processed: 7\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[-4.3401,  4.2232, -0.1511]]), hidden_states=None, attentions=None)\n",
            "entailment\n",
            "Total items processed: 8\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[-4.3565,  4.1549, -0.0151]]), hidden_states=None, attentions=None)\n",
            "entailment\n",
            "Total items processed: 9\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[-3.0250, -2.8605,  4.3458]]), hidden_states=None, attentions=None)\n",
            "neutral\n",
            "Total items processed: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Function to calculate the mean entailment probability and return entailment probabilities\n",
        "def calculate_mean_entailment_prob(Answer, Premise):\n",
        "    # Tokenize the Answer text into sentences\n",
        "    Hypothesis = sent_tokenize(Answer)\n",
        "\n",
        "    # Load the NLI model and tokenizer\n",
        "    model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-deberta-v3-large')\n",
        "    tokenizer = AutoTokenizer.from_pretrained('cross-encoder/nli-deberta-v3-large')\n",
        "\n",
        "    # Prepare the premise-hypothesis pairs\n",
        "    pairs = [(Premise, hypo) for hypo in Hypothesis]\n",
        "\n",
        "    # Tokenize the pairs\n",
        "    features = tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Model evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(**features).logits\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        entailment_probs = probs[:, 1].tolist()  # Extract the entailment probabilities\n",
        "\n",
        "    # Calculate the mean entailment probability\n",
        "    mean_entailment_prob = sum(entailment_probs) / len(entailment_probs) if entailment_probs else 0\n",
        "\n",
        "    return mean_entailment_prob, entailment_probs\n",
        "\n",
        "# Ensure nltk 'punkt' tokenizer data is available\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the JSON data from the file\n",
        "file_path = '/content/drive/Othercomputers/MBZUAI/MBZUAI/ADGM-Project/SharedTask/COBS_VER15.150823_answer_entailed_questions_subtask2_sample_with_labels.json'\n",
        "\n",
        "# Read the JSON data\n",
        "with open(file_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Initialize a counter\n",
        "item_count = 0\n",
        "\n",
        "# Process each item in the JSON data\n",
        "for item in data:\n",
        "    Answer = item[\"Answer\"]\n",
        "    Premise = item[\"Question\"]\n",
        "\n",
        "    # Calculate mean entailment probability and entailment probabilities for each pair\n",
        "    mean_entailment_prob, entailment_probs = calculate_mean_entailment_prob(Answer, Premise)\n",
        "\n",
        "    # Add the results back to the item in the JSON data\n",
        "    item[\"Mean_Entailment_Prob\"] = mean_entailment_prob\n",
        "    item[\"Entailment_Probs\"] = entailment_probs\n",
        "\n",
        "    # Increment the counter\n",
        "    item_count += 1\n",
        "\n",
        "    # Print the total number of items processed\n",
        "    print(f\"Total number of items processed: {item_count}\")\n",
        "\n",
        "# Save the updated JSON data back to a new file\n",
        "updated_file_path = '/content/drive/Othercomputers/MBZUAI/MBZUAI/ADGM-Project/SharedTask/COBS_VER15.150823_answer_entailed_questions_subtask2_sample_with_labels.json'\n",
        "with open(updated_file_path, 'w') as file:\n",
        "    json.dump(data, file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5XN8GWAdW4p",
        "outputId": "f2b38755-2dc0-4d04-a761-269377e08821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of items processed: 1\n",
            "Total number of items processed: 2\n",
            "Total number of items processed: 3\n",
            "Total number of items processed: 4\n",
            "Total number of items processed: 5\n",
            "Total number of items processed: 6\n",
            "Total number of items processed: 7\n",
            "Total number of items processed: 8\n",
            "Total number of items processed: 9\n",
            "Total number of items processed: 10\n",
            "Total number of items processed: 11\n",
            "Total number of items processed: 12\n",
            "Total number of items processed: 13\n",
            "Total number of items processed: 14\n",
            "Total number of items processed: 15\n",
            "Total number of items processed: 16\n",
            "Total number of items processed: 17\n",
            "Total number of items processed: 18\n",
            "Total number of items processed: 19\n",
            "Total number of items processed: 20\n",
            "Total number of items processed: 21\n",
            "Total number of items processed: 22\n",
            "Total number of items processed: 23\n",
            "Total number of items processed: 24\n",
            "Total number of items processed: 25\n",
            "Total number of items processed: 26\n",
            "Total number of items processed: 27\n",
            "Total number of items processed: 28\n",
            "Total number of items processed: 29\n",
            "Total number of items processed: 30\n",
            "Total number of items processed: 31\n",
            "Total number of items processed: 32\n",
            "Total number of items processed: 33\n",
            "Total number of items processed: 34\n",
            "Total number of items processed: 35\n",
            "Total number of items processed: 36\n",
            "Total number of items processed: 37\n",
            "Total number of items processed: 38\n",
            "Total number of items processed: 39\n",
            "Total number of items processed: 40\n",
            "Total number of items processed: 41\n",
            "Total number of items processed: 42\n",
            "Total number of items processed: 43\n",
            "Total number of items processed: 44\n",
            "Total number of items processed: 45\n",
            "Total number of items processed: 46\n",
            "Total number of items processed: 47\n",
            "Total number of items processed: 48\n",
            "Total number of items processed: 49\n",
            "Total number of items processed: 50\n",
            "Total number of items processed: 51\n",
            "Total number of items processed: 52\n",
            "Total number of items processed: 53\n",
            "Total number of items processed: 54\n",
            "Total number of items processed: 55\n",
            "Total number of items processed: 56\n",
            "Total number of items processed: 57\n",
            "Total number of items processed: 58\n",
            "Total number of items processed: 59\n",
            "Total number of items processed: 60\n",
            "Total number of items processed: 61\n",
            "Total number of items processed: 62\n",
            "Total number of items processed: 63\n",
            "Total number of items processed: 64\n",
            "Total number of items processed: 65\n",
            "Total number of items processed: 66\n",
            "Total number of items processed: 67\n"
          ]
        }
      ]
    }
  ]
}