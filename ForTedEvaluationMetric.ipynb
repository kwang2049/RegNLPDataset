{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1AM_WZmgBVWFKE1ob-y8VLU_PE2gvU0kH",
      "authorship_tag": "ABX9TyMb8afXlTDoWD3aAf/B8qae",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tubagokhan/RegNLPDataset/blob/main/ForTedEvaluationMetric.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvlPbL3VYB-S",
        "outputId": "a5bd6f0c-b0c7-463c-fddf-3ec7b7c11452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependencies imported and NLTK punkt downloaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLI model and tokenizer loaded.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import time\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "print(\"Dependencies imported and NLTK punkt downloaded.\")\n",
        "\n",
        "# Load NLI model\n",
        "nli_model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-deberta-v3-xsmall')\n",
        "tokenizer = AutoTokenizer.from_pretrained('cross-encoder/nli-deberta-v3-xsmall')\n",
        "\n",
        "print(\"NLI model and tokenizer loaded.\")\n",
        "\n",
        "def softmax(logits):\n",
        "    e_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "    return e_logits / np.sum(e_logits, axis=1, keepdims=True)\n",
        "\n",
        "def get_nli_probabilities(premises, hypotheses):\n",
        "    \"\"\"\n",
        "    Computes the Natural Language Inference (NLI) probabilities for a set of premise-hypothesis pairs.\n",
        "\n",
        "    This function tokenizes the input premises and hypotheses, feeds them into a pre-trained NLI model, and\n",
        "    returns the softmax probabilities of the model's logits. The probabilities indicate how likely it is\n",
        "    that the relationship between each premise and its corresponding hypothesis is entailment, neutral, or contradiction.\n",
        "\n",
        "    Parameters:\n",
        "    premises (list of str): The premises to evaluate, where each premise is a statement that is assumed to be true.\n",
        "    hypotheses (list of str): The hypotheses to evaluate against the premises, where each hypothesis is a statement whose truth value is being assessed in the context of the premises.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: A 2D array of shape (number of pairs, 3), containing the rounded probabilities for entailment, neutral, and contradiction for each premise-hypothesis pair.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the premises and hypotheses for input into the NLI model\n",
        "    features = tokenizer(premises, hypotheses, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Set the model to evaluation mode to disable dropout and batch normalization\n",
        "    nli_model.eval()\n",
        "\n",
        "    # Disable gradient calculation to speed up the process and reduce memory usage\n",
        "    with torch.no_grad():\n",
        "        # Get the raw logits from the NLI model\n",
        "        logits = nli_model(**features).logits.numpy()\n",
        "\n",
        "    # Apply the softmax function to the logits to get probabilities\n",
        "    probabilities = softmax(logits)\n",
        "\n",
        "    # Round the probabilities to three decimal places for readability\n",
        "    return np.round(probabilities, 3)\n",
        "\n",
        "\n",
        "def get_nli_matrix(passages, answers):\n",
        "    \"\"\"\n",
        "    Generates two matrices representing entailment and contradiction probabilities between a list of passages and answers.\n",
        "\n",
        "    This function uses a Natural Language Inference (NLI) model to compute the probabilities that each answer\n",
        "    entails or contradicts each passage. The entailment and contradiction probabilities are stored in separate matrices.\n",
        "\n",
        "    Parameters:\n",
        "    passages (list of str): The list of text passages.\n",
        "    answers (list of str): The list of answers to evaluate against the passages.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing two numpy arrays - the entailment matrix and the contradiction matrix.\n",
        "    \"\"\"\n",
        "\n",
        "    # Log the number of passages and answers being processed\n",
        "    print(f\"{len(passages)} passages and {len(answers)} answers.\")\n",
        "\n",
        "    # Initialize matrices to store entailment and contradiction probabilities\n",
        "    entailment_matrix = np.zeros((len(passages), len(answers)))\n",
        "    contradiction_matrix = np.zeros((len(passages), len(answers)))\n",
        "\n",
        "    # Iterate over each passage and answer pair\n",
        "    for i, pas in enumerate(passages):\n",
        "        for j, ans in enumerate(answers):\n",
        "            # Get the NLI probabilities for the current passage-answer pair\n",
        "            probs = get_nli_probabilities([pas], [ans])\n",
        "\n",
        "            # Optional logging for debugging or analysis\n",
        "            print(pas)\n",
        "            print(ans)\n",
        "            print(probs)\n",
        "            print(\"---\")\n",
        "\n",
        "            # Update the entailment matrix with the probability of entailment\n",
        "            entailment_matrix[i, j] = probs[:, 1][0]  # Access the first element to avoid deprecation warning\n",
        "\n",
        "            # Update the contradiction matrix with the probability of contradiction\n",
        "            contradiction_matrix[i, j] = probs[:, 0][0]  # Access the first element to avoid deprecation warning\n",
        "\n",
        "    # Optional logging of the final matrices for debugging or analysis\n",
        "    print(\"Entailment Matrix\")\n",
        "    print(entailment_matrix)\n",
        "    print(\"Contradiction Matrix\")\n",
        "    print(contradiction_matrix)\n",
        "\n",
        "    # Return both the entailment and contradiction matrices\n",
        "    return entailment_matrix, contradiction_matrix\n",
        "\n",
        "\n",
        "def calculate_scores_from_matrix(nli_matrix, score_type='entailment'):\n",
        "    \"\"\"\n",
        "    Calculates a score from an NLI (Natural Language Inference) matrix based on the specified score type.\n",
        "\n",
        "    The function supports calculating scores for 'entailment' or 'contradiction' by reducing the NLI matrix\n",
        "    along a specific axis and then computing the mean of the reduced vector. The score is rounded to three decimal places.\n",
        "\n",
        "    Parameters:\n",
        "    nli_matrix (numpy.ndarray): The NLI matrix containing inference scores.\n",
        "    score_type (str): The type of score to calculate, 'entailment' or 'contradiction'. Defaults to 'entailment'.\n",
        "\n",
        "    Returns:\n",
        "    float: The calculated score, rounded to three decimal places.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if the NLI matrix is empty and return a default score if true\n",
        "    if nli_matrix.size == 0:\n",
        "        print(\"Warning: NLI matrix is empty. Returning default score of 0.\")\n",
        "        return 0.0  # Return a default score of 0.0 if the NLI matrix is empty\n",
        "\n",
        "    # Reduce the NLI matrix to a vector based on the score type\n",
        "    if score_type == 'entailment':\n",
        "        reduced_vector = np.max(nli_matrix, axis=0)  # Max reduction for entailment scores\n",
        "    elif score_type == 'contradiction':\n",
        "        reduced_vector = np.max(nli_matrix, axis=0)  # Max reduction for contradiction scores (same as entailment in this implementation)\n",
        "\n",
        "    # Calculate the mean of the reduced vector and round it to three decimal places\n",
        "    score = np.round(np.mean(reduced_vector), 3)\n",
        "\n",
        "    return score  # Return the calculated score\n",
        "\n",
        "\n",
        "def check_obligation(text):\n",
        "    \"\"\"\n",
        "    Determines whether a given text contains an obligation by predicting it using a trained machine learning model.\n",
        "\n",
        "    The function loads a dataset of texts and their associated obligation labels from a JSON file,\n",
        "    trains a Logistic Regression model using TF-IDF features on a subset of the data, and then predicts\n",
        "    whether the input text contains an obligation.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): The text to be checked for an obligation.\n",
        "\n",
        "    Returns:\n",
        "    int: The predicted label indicating the presence (1) or absence (0) of an obligation in the text.\n",
        "    \"\"\"\n",
        "\n",
        "    # Path to the JSON file containing the dataset\n",
        "    json_path = \"/content/drive/Othercomputers/MBZUAI/MBZUAI/ADGM-Project/SharedTask/Processed_Obligations.json\"\n",
        "\n",
        "    # Open and load the dataset from the JSON file\n",
        "    with open(json_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Extract texts and their corresponding obligation labels from the dataset\n",
        "    texts = [item['Text'] for item in data]\n",
        "    labels = [item['Obligation'] for item in data]\n",
        "\n",
        "    # Split the dataset into training and unused test sets (test set is unused here)\n",
        "    X_train, _, y_train, _ = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define a machine learning pipeline consisting of TF-IDF vectorization followed by Logistic Regression\n",
        "    pipeline = make_pipeline(TfidfVectorizer(stop_words='english'), LogisticRegression(max_iter=1000))\n",
        "\n",
        "    # Train the pipeline on the training data\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and return the obligation label for the input text (1 for presence, 0 for absence of obligation)\n",
        "    return pipeline.predict([text])[0]\n",
        "\n",
        "\n",
        "def calculate_coverage_score(passages, answers):\n",
        "    \"\"\"\n",
        "    Calculate the coverage score indicating the extent to which the provided answers cover the obligations mentioned in the passages.\n",
        "\n",
        "    Parameters:\n",
        "    passages (list of str): The text passages containing obligations.\n",
        "    answers (list of str): The answers that are supposed to cover the obligations in the passages.\n",
        "\n",
        "    Returns:\n",
        "    float: The coverage score, rounded to three decimal places, representing the proportion of obligations covered by the answers.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the counter for covered obligations\n",
        "    covered_obligations = 0\n",
        "\n",
        "    # Calculate the total number of obligations in all passages\n",
        "    total_obligations = sum(\n",
        "        check_obligation(sent)  # Check if a sentence contains an obligation\n",
        "        for passage in passages  # Iterate over each passage\n",
        "        for sent in sent_tokenize(passage)  # Tokenize the passage into sentences\n",
        "    )\n",
        "\n",
        "    # Iterate over each answer and passage to check for covered obligations\n",
        "    for ans in answers:\n",
        "        for passage in passages:\n",
        "            for sent in sent_tokenize(passage):  # Tokenize the passage into sentences\n",
        "                # If the sentence contains an obligation, it is considered covered\n",
        "                if check_obligation(sent):\n",
        "                    print(f\"Obligation: {sent}\")  # Optional: Print the obligation sentence\n",
        "                    covered_obligations += 1  # Increment the counter for covered obligations\n",
        "                    break  # Move to the next passage/answer after covering an obligation\n",
        "\n",
        "    # Calculate the coverage score as the ratio of covered obligations to total obligations\n",
        "    coverage_score = (\n",
        "        covered_obligations / total_obligations if total_obligations > 0 else 0\n",
        "    )\n",
        "\n",
        "    # Round the coverage score to three decimal places for reporting\n",
        "    coverage_score_rounded = np.round(coverage_score, 3)\n",
        "    print(f\"Obligation Coverage score: {coverage_score_rounded}\")  # Print the rounded coverage score\n",
        "\n",
        "    return coverage_score_rounded  # Return the rounded coverage score\n",
        "\n",
        "\n",
        "def calculate_final_composite_score(passages, answers, we=0.4, wc=0.3, wo=0.3):\n",
        "    #print(\"Calculating final composite score.\")\n",
        "    passage_sentences = [sent for passage in passages for sent in sent_tokenize(passage)]\n",
        "    answer_sentences = [sent for answer in answers for sent in sent_tokenize(answer)]\n",
        "    entailment_matrix, contradiction_matrix = get_nli_matrix(passage_sentences, answer_sentences)\n",
        "\n",
        "    entailment_score = calculate_scores_from_matrix(entailment_matrix, 'entailment')\n",
        "    print(f\"Entailment score: {np.round(entailment_score, 3)}\")\n",
        "    contradiction_score = calculate_scores_from_matrix(contradiction_matrix, 'contradiction')\n",
        "    print(f\"Contradiction score: {np.round(contradiction_score, 3)}\")\n",
        "    coverage_score = calculate_coverage_score(passages, answers)\n",
        "\n",
        "    composite_score = we * entailment_score - wc * contradiction_score + wo * coverage_score\n",
        "    print(f\"Final composite score: {np.round(composite_score, 3)}\")\n",
        "    return np.round(composite_score, 3)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def calculate_conciseness_penalty(retrieved_passage, answer):\n",
        "    \"\"\"\n",
        "    Calculates the overlap ratio between words in the retrieved passage and the answer.\n",
        "    This ratio is used to penalize the conciseness score of the answer, based on the amount\n",
        "    of verbatim text copied from the source.\n",
        "\n",
        "    Parameters:\n",
        "    - retrieved_passage (str): The source text from which information is retrieved.\n",
        "    - answer (str): The respondent's answer that needs to be evaluated for conciseness.\n",
        "\n",
        "    Returns:\n",
        "    - float: The overlap ratio, representing the fraction of the answer's content that is\n",
        "      directly copied from the retrieved passage. A higher ratio indicates more copying,\n",
        "      which can be used to penalize the conciseness score.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize words in both the retrieved passage and the answer, converting to lowercase\n",
        "    retrieved_tokens = re.findall(r'\\w+', retrieved_passage.lower())\n",
        "    answer_tokens = re.findall(r'\\w+', answer.lower())\n",
        "\n",
        "    # Count occurrences of each word in the answer\n",
        "    answer_word_count = Counter(answer_tokens)\n",
        "\n",
        "    # Initialize the count of overlapping words\n",
        "    overlap_count = 0\n",
        "\n",
        "    # Iterate over unique words in the retrieved passage and count how many are in the answer\n",
        "    for word in set(retrieved_tokens):\n",
        "        if word in answer_word_count:\n",
        "            overlap_count += min(retrieved_tokens.count(word), answer_word_count[word])\n",
        "\n",
        "    # Calculate the total number of words in the answer\n",
        "    total_words_in_answer = len(answer_tokens)\n",
        "\n",
        "    # Calculate the overlap ratio\n",
        "    overlap_ratio = overlap_count / total_words_in_answer if total_words_in_answer > 0 else 0\n",
        "\n",
        "    print(f\"OVerlap Ratio: {np.round(overlap_ratio, 3)}\")\n",
        "    return overlap_ratio"
      ],
      "metadata": {
        "id": "qP3TcAcnrFyn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def calculate_normalized_overlap_score(retrieved_passage, answer):\n",
        "    \"\"\"\n",
        "    Calculates the normalized overlap score between the retrieved passage and the answer.\n",
        "    This score reflects the proportion of words in the answer that are also found in the retrieved passage,\n",
        "    normalized by the total number of words in the answer.\n",
        "\n",
        "    Parameters:\n",
        "    - retrieved_passage (str): The source text from which information is retrieved.\n",
        "    - answer (str): The respondent's answer that needs to be evaluated for originality.\n",
        "\n",
        "    Returns:\n",
        "    - float: The normalized overlap score, representing the proportion of the answer's content\n",
        "      that overlaps with the retrieved passage. A higher score indicates more overlap.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize words in both the retrieved passage and the answer, converting to lowercase\n",
        "    retrieved_tokens = re.findall(r'\\w+', retrieved_passage.lower())\n",
        "    answer_tokens = re.findall(r'\\w+', answer.lower())\n",
        "\n",
        "    # Create sets of unique words from both the retrieved passage and the answer\n",
        "    unique_retrieved_tokens = set(retrieved_tokens)\n",
        "    unique_answer_tokens = set(answer_tokens)\n",
        "\n",
        "    # Calculate the overlap by finding the intersection of unique words in both sets\n",
        "    overlap = unique_retrieved_tokens.intersection(unique_answer_tokens)\n",
        "\n",
        "    # Count the occurrences of overlapping words in the answer\n",
        "    answer_word_count = Counter(answer_tokens)\n",
        "    overlap_count = sum(answer_word_count[word] for word in overlap)\n",
        "\n",
        "    # Calculate the total number of words in the answer\n",
        "    total_words_in_answer = len(answer_tokens)\n",
        "\n",
        "    # Calculate the normalized overlap score\n",
        "    normalized_overlap_score = overlap_count / total_words_in_answer if total_words_in_answer > 0 else 0\n",
        "\n",
        "    print(f'Normalized Overlap Score: {normalized_overlap_score:.2f}')\n",
        "    return normalized_overlap_score\n",
        "\n"
      ],
      "metadata": {
        "id": "OZhLmIXouv-N"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Question= \"Can a Model Portfolio that follows an environmentally-conscious investment mandate use the ADGM Green Portfolio or ADGM Climate Transition Portfolio designation without obtaining the relevant designation?\"\n",
        "RetrievedPassage=[\"A Model Portfolio that follows an environmentally-conscious investment mandate is not required to become an ADGM Green Portfolio or an ADGM Climate Transition Portfolio. However, only those Model Portfolios which have obtained the relevant designation may use such terminology and designation marks in related documentation and promotional materials.\"]\n",
        "Answer=[\"No, a Model Portfolio that follows an environmentally-conscious investment mandate is not permitted to use the ADGM Green Portfolio or ADGM Climate Transition Portfolio designation without obtaining the relevant designation. Only Model Portfolios that have received the official designation are allowed to use such terminology and designation marks in their documentation and promotional materials.\"]\n",
        "calculate_final_composite_score(RetrievedPassage, Answer)\n",
        "calculate_conciseness_penalty(RetrievedPassage[0], Answer[0])\n",
        "temp=calculate_normalized_overlap_score(RetrievedPassage[0], Answer[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9eZZnuU5aT-",
        "outputId": "f7560ab3-f84e-4dcc-f94a-d1215a46b567"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 passages and 2 answers.\n",
            "A Model Portfolio that follows an environmentally-conscious investment mandate is not required to become an ADGM Green Portfolio or an ADGM Climate Transition Portfolio.\n",
            "No, a Model Portfolio that follows an environmentally-conscious investment mandate is not permitted to use the ADGM Green Portfolio or ADGM Climate Transition Portfolio designation without obtaining the relevant designation.\n",
            "[[0.988 0.003 0.01 ]]\n",
            "---\n",
            "A Model Portfolio that follows an environmentally-conscious investment mandate is not required to become an ADGM Green Portfolio or an ADGM Climate Transition Portfolio.\n",
            "Only Model Portfolios that have received the official designation are allowed to use such terminology and designation marks in their documentation and promotional materials.\n",
            "[[0.948 0.    0.052]]\n",
            "---\n",
            "However, only those Model Portfolios which have obtained the relevant designation may use such terminology and designation marks in related documentation and promotional materials.\n",
            "No, a Model Portfolio that follows an environmentally-conscious investment mandate is not permitted to use the ADGM Green Portfolio or ADGM Climate Transition Portfolio designation without obtaining the relevant designation.\n",
            "[[0.108 0.006 0.886]]\n",
            "---\n",
            "However, only those Model Portfolios which have obtained the relevant designation may use such terminology and designation marks in related documentation and promotional materials.\n",
            "Only Model Portfolios that have received the official designation are allowed to use such terminology and designation marks in their documentation and promotional materials.\n",
            "[[0.001 0.984 0.015]]\n",
            "---\n",
            "Entailment Matrix\n",
            "[[0.003      0.        ]\n",
            " [0.006      0.98400003]]\n",
            "Contradiction Matrix\n",
            "[[0.98799998 0.94800001]\n",
            " [0.108      0.001     ]]\n",
            "Entailment score: 0.495\n",
            "Contradiction score: 0.968\n",
            "Obligation: A Model Portfolio that follows an environmentally-conscious investment mandate is not required to become an ADGM Green Portfolio or an ADGM Climate Transition Portfolio.\n",
            "Obligation Coverage score: 0.5\n",
            "Final composite score: 0.058\n",
            "OVerlap Ratio: 0.709\n",
            "Normalized Overlap Score: 0.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Question=[\"In determining which economic activities are environmentally sustainable, what accreditation standards are considered credible by the Regulator?\"]\n",
        "RetrievedPassage=[\"The Regulator considers an Acceptable Green Taxonomy to be one which uses credible and independent accreditation standards published by a governmental body or a reputable industry association and made publicly available to allow determination of which economic activities are environmentally sustainable.  Examples of green taxonomies that are considered Acceptable Green Taxonomies include:a.The EU Green Taxonomy; b.The ASEAN Taxonomy for Sustainable Finance; and c.The Common Principles for Climate Mitigation Finance Tracking.\"]\n",
        "Answer=[\"The Regulator considers accreditation standards published by a governmental body or a reputable industry association as credible for determining which economic activities are environmentally sustainable. These accreditation standards should also be made publicly available to allow transparency and enable stakeholders to assess the sustainability of economic activities effectively. Examples of green taxonomies that meet these criteria and are considered Acceptable Green Taxonomies by the Regulator include the EU Green Taxonomy, the ASEAN Taxonomy for Sustainable Finance, and the Common Principles for Climate Mitigation Finance Tracking.\"]\n",
        "calculate_final_composite_score(RetrievedPassage, Answer)\n",
        "calculate_conciseness_penalty(RetrievedPassage[0], Answer[0])\n",
        "temp=calculate_normalized_overlap_score(RetrievedPassage[0], Answer[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "am5nBf0_lTr4",
        "outputId": "39ab5e31-7075-42b6-d10e-2b1f9bd0ff2d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 passages and 3 answers.\n",
            "The Regulator considers an Acceptable Green Taxonomy to be one which uses credible and independent accreditation standards published by a governmental body or a reputable industry association and made publicly available to allow determination of which economic activities are environmentally sustainable.\n",
            "The Regulator considers accreditation standards published by a governmental body or a reputable industry association as credible for determining which economic activities are environmentally sustainable.\n",
            "[[0.    0.995 0.005]]\n",
            "---\n",
            "The Regulator considers an Acceptable Green Taxonomy to be one which uses credible and independent accreditation standards published by a governmental body or a reputable industry association and made publicly available to allow determination of which economic activities are environmentally sustainable.\n",
            "These accreditation standards should also be made publicly available to allow transparency and enable stakeholders to assess the sustainability of economic activities effectively.\n",
            "[[0.    0.001 0.998]]\n",
            "---\n",
            "The Regulator considers an Acceptable Green Taxonomy to be one which uses credible and independent accreditation standards published by a governmental body or a reputable industry association and made publicly available to allow determination of which economic activities are environmentally sustainable.\n",
            "Examples of green taxonomies that meet these criteria and are considered Acceptable Green Taxonomies by the Regulator include the EU Green Taxonomy, the ASEAN Taxonomy for Sustainable Finance, and the Common Principles for Climate Mitigation Finance Tracking.\n",
            "[[0.001 0.001 0.999]]\n",
            "---\n",
            "Examples of green taxonomies that are considered Acceptable Green Taxonomies include:a.The EU Green Taxonomy; b.The ASEAN Taxonomy for Sustainable Finance; and c.The Common Principles for Climate Mitigation Finance Tracking.\n",
            "The Regulator considers accreditation standards published by a governmental body or a reputable industry association as credible for determining which economic activities are environmentally sustainable.\n",
            "[[0.002 0.017 0.981]]\n",
            "---\n",
            "Examples of green taxonomies that are considered Acceptable Green Taxonomies include:a.The EU Green Taxonomy; b.The ASEAN Taxonomy for Sustainable Finance; and c.The Common Principles for Climate Mitigation Finance Tracking.\n",
            "These accreditation standards should also be made publicly available to allow transparency and enable stakeholders to assess the sustainability of economic activities effectively.\n",
            "[[0.001 0.001 0.999]]\n",
            "---\n",
            "Examples of green taxonomies that are considered Acceptable Green Taxonomies include:a.The EU Green Taxonomy; b.The ASEAN Taxonomy for Sustainable Finance; and c.The Common Principles for Climate Mitigation Finance Tracking.\n",
            "Examples of green taxonomies that meet these criteria and are considered Acceptable Green Taxonomies by the Regulator include the EU Green Taxonomy, the ASEAN Taxonomy for Sustainable Finance, and the Common Principles for Climate Mitigation Finance Tracking.\n",
            "[[0.    0.007 0.993]]\n",
            "---\n",
            "Entailment Matrix\n",
            "[[0.995 0.001 0.001]\n",
            " [0.017 0.001 0.007]]\n",
            "Contradiction Matrix\n",
            "[[0.    0.    0.001]\n",
            " [0.002 0.001 0.   ]]\n",
            "Entailment score: 0.334\n",
            "Contradiction score: 0.001\n",
            "Obligation: The Regulator considers an Acceptable Green Taxonomy to be one which uses credible and independent accreditation standards published by a governmental body or a reputable industry association and made publicly available to allow determination of which economic activities are environmentally sustainable.\n",
            "Obligation Coverage score: 0.5\n",
            "Final composite score: 0.283\n",
            "OVerlap Ratio: 0.729\n",
            "Normalized Overlap Score: 0.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question= \"Can a Model Portfolio that follows an environmentally-conscious investment mandate use the ADGM Green Portfolio or ADGM Climate Transition Portfolio designation without obtaining the relevant designation?\"\n",
        "#RetrievedPassage=[\"A Model Portfolio that follows an environmentally-conscious investment mandate is not required to become an ADGM Green Portfolio or an ADGM Climate Transition Portfolio. However, only those Model Portfolios which have obtained the relevant designation may use such terminology and designation marks in related documentation and promotional materials.\"]\n",
        "#Answer=[\"No, a Model Portfolio that follows an environmentally-conscious investment mandate is not permitted to use the ADGM Green Portfolio or ADGM Climate Transition Portfolio designation without obtaining the relevant designation. Only Model Portfolios that have received the official designation are allowed to use such terminology and designation marks in their documentation and promotional materials.\"]\n",
        "\n",
        "Question=[\"In determining which economic activities are environmentally sustainable, what accreditation standards are considered credible by the Regulator?\"]\n",
        "RetrievedPassage=[\"The Regulator considers an Acceptable Green Taxonomy to be one which uses credible and independent accreditation standards published by a governmental body or a reputable industry association and made publicly available to allow determination of which economic activities are environmentally sustainable.  Examples of green taxonomies that are considered Acceptable Green Taxonomies include:a.The EU Green Taxonomy; b.The ASEAN Taxonomy for Sustainable Finance; and c.The Common Principles for Climate Mitigation Finance Tracking.\"]\n",
        "Answer=[\"The Regulator considers accreditation standards published by a governmental body or a reputable industry association as credible for determining which economic activities are environmentally sustainable. In an effort to enhance transparency and inclusivity, the Regulator has also begun integrating elements from the Atlantean Sustainability Framework, a set of guidelines rumored to originate from an ancient, advanced civilization, focusing on harmonizing human activities with the natural world. These accreditation standards should be made publicly available to allow transparency and enable stakeholders to assess the sustainability of economic activities effectively. To further this goal, the Regulator has introduced the Ethereal Compliance System, an innovative digital platform that employs spectral analysis to measure the environmental impact of economic activities, providing a more intuitive understanding of sustainability metrics. Examples of green taxonomies that meet these criteria and are considered Acceptable Green Taxonomies by the Regulator include the EU Green Taxonomy, the ASEAN Taxonomy for Sustainable Finance, the Common Principles for Climate Mitigation Finance Tracking, and the newly added Arcadian Environmental Codex, believed to offer insights into ancient ecological wisdom that aligns modern economic practices with timeless natural principles.\"]\n",
        "calculate_final_composite_score(RetrievedPassage, Answer)\n",
        "calculate_conciseness_penalty(RetrievedPassage[0], Answer[0])\n",
        "temp=calculate_normalized_overlap_score(RetrievedPassage[0], Answer[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GHUxROFYLAY",
        "outputId": "5f6d1a02-7644-4735-c7ae-28169800b0ca"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 passages and 5 answers.\n",
            "The Regulator considers an Acceptable Green Taxonomy to be one which uses credible and independent accreditation standards published by a governmental body or a reputable industry association and made publicly available to allow determination of which economic activities are environmentally sustainable.\n",
            "The Regulator considers accreditation standards published by a governmental body or a reputable industry association as credible for determining which economic activities are environmentally sustainable.\n",
            "[[0.    0.995 0.005]]\n",
            "---\n",
            "The Regulator considers an Acceptable Green Taxonomy to be one which uses credible and independent accreditation standards published by a governmental body or a reputable industry association and made publicly available to allow determination of which economic activities are environmentally sustainable.\n",
            "In an effort to enhance transparency and inclusivity, the Regulator has also begun integrating elements from the Atlantean Sustainability Framework, a set of guidelines rumored to originate from an ancient, advanced civilization, focusing on harmonizing human activities with the natural world.\n",
            "[[0.001 0.001 0.998]]\n",
            "---\n",
            "The Regulator considers an Acceptable Green Taxonomy to be one which uses credible and independent accreditation standards published by a governmental body or a reputable industry association and made publicly available to allow determination of which economic activities are environmentally sustainable.\n",
            "These accreditation standards should be made publicly available to allow transparency and enable stakeholders to assess the sustainability of economic activities effectively.\n",
            "[[0.001 0.013 0.986]]\n",
            "---\n",
            "The Regulator considers an Acceptable Green Taxonomy to be one which uses credible and independent accreditation standards published by a governmental body or a reputable industry association and made publicly available to allow determination of which economic activities are environmentally sustainable.\n",
            "To further this goal, the Regulator has introduced the Ethereal Compliance System, an innovative digital platform that employs spectral analysis to measure the environmental impact of economic activities, providing a more intuitive understanding of sustainability metrics.\n",
            "[[0.001 0.    0.998]]\n",
            "---\n",
            "The Regulator considers an Acceptable Green Taxonomy to be one which uses credible and independent accreditation standards published by a governmental body or a reputable industry association and made publicly available to allow determination of which economic activities are environmentally sustainable.\n",
            "Examples of green taxonomies that meet these criteria and are considered Acceptable Green Taxonomies by the Regulator include the EU Green Taxonomy, the ASEAN Taxonomy for Sustainable Finance, the Common Principles for Climate Mitigation Finance Tracking, and the newly added Arcadian Environmental Codex, believed to offer insights into ancient ecological wisdom that aligns modern economic practices with timeless natural principles.\n",
            "[[0.002 0.001 0.998]]\n",
            "---\n",
            "Examples of green taxonomies that are considered Acceptable Green Taxonomies include:a.The EU Green Taxonomy; b.The ASEAN Taxonomy for Sustainable Finance; and c.The Common Principles for Climate Mitigation Finance Tracking.\n",
            "The Regulator considers accreditation standards published by a governmental body or a reputable industry association as credible for determining which economic activities are environmentally sustainable.\n",
            "[[0.002 0.017 0.981]]\n",
            "---\n",
            "Examples of green taxonomies that are considered Acceptable Green Taxonomies include:a.The EU Green Taxonomy; b.The ASEAN Taxonomy for Sustainable Finance; and c.The Common Principles for Climate Mitigation Finance Tracking.\n",
            "In an effort to enhance transparency and inclusivity, the Regulator has also begun integrating elements from the Atlantean Sustainability Framework, a set of guidelines rumored to originate from an ancient, advanced civilization, focusing on harmonizing human activities with the natural world.\n",
            "[[0.001 0.001 0.998]]\n",
            "---\n",
            "Examples of green taxonomies that are considered Acceptable Green Taxonomies include:a.The EU Green Taxonomy; b.The ASEAN Taxonomy for Sustainable Finance; and c.The Common Principles for Climate Mitigation Finance Tracking.\n",
            "These accreditation standards should be made publicly available to allow transparency and enable stakeholders to assess the sustainability of economic activities effectively.\n",
            "[[0.001 0.001 0.999]]\n",
            "---\n",
            "Examples of green taxonomies that are considered Acceptable Green Taxonomies include:a.The EU Green Taxonomy; b.The ASEAN Taxonomy for Sustainable Finance; and c.The Common Principles for Climate Mitigation Finance Tracking.\n",
            "To further this goal, the Regulator has introduced the Ethereal Compliance System, an innovative digital platform that employs spectral analysis to measure the environmental impact of economic activities, providing a more intuitive understanding of sustainability metrics.\n",
            "[[0.001 0.001 0.998]]\n",
            "---\n",
            "Examples of green taxonomies that are considered Acceptable Green Taxonomies include:a.The EU Green Taxonomy; b.The ASEAN Taxonomy for Sustainable Finance; and c.The Common Principles for Climate Mitigation Finance Tracking.\n",
            "Examples of green taxonomies that meet these criteria and are considered Acceptable Green Taxonomies by the Regulator include the EU Green Taxonomy, the ASEAN Taxonomy for Sustainable Finance, the Common Principles for Climate Mitigation Finance Tracking, and the newly added Arcadian Environmental Codex, believed to offer insights into ancient ecological wisdom that aligns modern economic practices with timeless natural principles.\n",
            "[[0.001 0.002 0.998]]\n",
            "---\n",
            "Entailment Matrix\n",
            "[[0.995 0.001 0.013 0.    0.001]\n",
            " [0.017 0.001 0.001 0.001 0.002]]\n",
            "Contradiction Matrix\n",
            "[[0.    0.001 0.001 0.001 0.002]\n",
            " [0.002 0.001 0.001 0.001 0.001]]\n",
            "Entailment score: 0.202\n",
            "Contradiction score: 0.001\n",
            "Obligation: The Regulator considers an Acceptable Green Taxonomy to be one which uses credible and independent accreditation standards published by a governmental body or a reputable industry association and made publicly available to allow determination of which economic activities are environmentally sustainable.\n",
            "Obligation Coverage score: 0.5\n",
            "Final composite score: 0.23\n",
            "OVerlap Ratio: 0.348\n",
            "Normalized Overlap Score: 0.55\n"
          ]
        }
      ]
    }
  ]
}